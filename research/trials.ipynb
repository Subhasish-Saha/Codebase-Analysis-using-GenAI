{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "from langchain.text_splitter import Language\n",
    "from langchain.document_loaders.generic import GenericLoader\n",
    "from langchain.document_loaders.parsers import LanguageParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains import ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clone the Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/subhasish/Documents/iNeuron/GenAI/Codebase-Analysis-using-GenAI/research'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir test_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<git.repo.base.Repo '/Users/subhasish/Documents/iNeuron/GenAI/Codebase-Analysis-using-GenAI/research/test_repo/.git'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "Repo.clone_from(\"https://github.com/Subhasish-Saha/Student-Performance-Indicator\", to_path=repo_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_path = \"test_repo/\"\n",
    "\n",
    "loader = GenericLoader.from_filesystem(repo_path+'/src',\n",
    "                                        glob = \"**/*\",\n",
    "                                       suffixes=[\".py\"],\n",
    "                                       parser = LanguageParser(language=Language.PYTHON, parser_threshold=500)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"import sys \\nfrom src.logger import logging\\n\\ndef error_message_detail(error, error_detail:sys):\\n\\n    _,_, exc_tb = error_detail.exc_info()\\n    filename = exc_tb.tb_frame.f_code.co_filename\\n    line_no = exc_tb.tb_lineno\\n    error_message = f'Error occured in the python script named : {filename} , at line no. : {line_no} , error details : {error} '\\n\\n    return error_message\\n\\nclass Custom_Exception(Exception):\\n\\n    def __init__(self, error_message, error_detail:sys):\\n        super().__init__(error_message)\\n        self.error_message = error_message_detail(\\n          error= error_message,\\n          error_detail=error_detail  \\n        )\\n\\n    def __str__(self) -> str:\\n        return self.error_message\", metadata={'source': 'test_repo/src/exception.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo/src/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport logging\\nimport sys\\nfrom datetime import datetime\\n\\nlogging_str = \"[ %(asctime)s ] %(lineno)d %(name)s - %(levelname)s - %(message)s\"\\nLOG_FILE = f\"{datetime.now().strftime(\\'%Y-%m-%d %H:%M:%S\\')}.log\"\\nlog_path = os.path.join(os.getcwd(),\"logs\",LOG_FILE)\\nos.makedirs(log_path, exist_ok=True)\\n\\nLOG_FILE_PATH = os.path.join(log_path,LOG_FILE)\\n\\nlogging.basicConfig(\\n    filename=LOG_FILE_PATH,\\n    format=logging_str,\\n    level = logging.INFO,\\n)', metadata={'source': 'test_repo/src/logger.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport pickle\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.model_selection import GridSearchCV\\nfrom sklearn.metrics import r2_score\\nfrom src.exception import Custom_Exception\\nfrom src.logger import logging\\n\\n\\ndef save_object(file_path: str, obj: object) -> None:\\n    \"\"\"\\n    Save the given object to a file at the specified file path.\\n\\n    Args:\\n        file_path (str): The path to the file where the object will be saved.\\n        obj (object): The object to be saved.\\n\\n    Raises:\\n        Custom_Exception: If an error occurs during the saving process, a Custom_Exception is raised.\\n\\n    Note:\\n        This function creates any necessary directories in the file path if they don\\'t exist.\\n    \"\"\"\\n    try:\\n        dir_path = os.path.dirname(file_path)\\n        os.makedirs(dir_path, exist_ok=True)\\n        with open(file_path, \"wb\") as file_obj:\\n            pickle.dump(obj, file_obj)\\n\\n    except Exception as e:\\n        raise Custom_Exception(e, sys)\\n\\n\\ndef load_object(file_path: str) -> pickle:\\n    \"\"\"\\n    Load an object from a file.\\n\\n    Args:\\n        file_path (str): The path to the file from which to load the object.\\n\\n    Returns:\\n        object: The loaded object.\\n\\n    Raises:\\n        Custom_Exception: If an error occurs while loading the object, a Custom_Exception is raised.\\n    \"\"\"\\n    try:\\n        with open(file_path, \"rb\") as file_obj:\\n            return pickle.load(file_obj)\\n\\n    except Exception as e:\\n        raise Custom_Exception(e, sys)\\n\\n\\ndef evaluate_models(\\n    X_train: np.array,\\n    y_train: np.array,\\n    X_test: np.array,\\n    y_test: np.array,\\n    models: dict,\\n    params: dict,\\n) -> float:\\n    \"\"\"\\n    Evaluate machine learning models using hyperparameter tuning and return a dictionary\\n    of model names and their corresponding R2 scores on the test data.\\n\\n    Parameters:\\n        X_train (np.array): The training data features.\\n        y_train (np.array): The training data labels.\\n        X_test (np.array): The test data features.\\n        y_test (np.array): The test data labels.\\n        models (dict): A dictionary of machine learning models, where keys are model names\\n                    and values are model objects.\\n        params (dict): A dictionary specifying hyperparameter search spaces for each model.\\n\\n    Returns:\\n        dict: A dictionary containing model names as keys and their R2 scores on the test data as values.\\n\\n    The function performs the following steps:\\n    1. Iterates through the provided list of models.\\n    2. Performs hyperparameter tuning for each model using GridSearchCV with cross-validation.\\n    3. Trains each model with the best hyperparameters on the training data.\\n    4. Calculates and stores the R2 score of each model on both the training and test data.\\n    5. Returns a dictionary where model names are keys and R2 scores on the test data are values.\\n    \"\"\"\\n    try:\\n        report = {}\\n\\n        for i in range(len(list(models))):\\n            model = list(models.values())[i]\\n            param = params[list(models.keys())[i]]\\n            logging.info(\\n                f\"Hyper parameter tuning for {list(models.values())[i]} started.\"\\n            )\\n            gs = GridSearchCV(model, param, cv=3, n_jobs=-1)\\n            gs.fit(X_train, y_train)\\n            model.set_params(**gs.best_params_)\\n            logging.info(\\n                f\"Hyper parameter tuned for {list(models.values())[i]} and the best parameters are used for model training.\"\\n            )\\n            model.fit(X_train, y_train)\\n\\n            # model.fit(X_train, y_train)  # Train model\\n            y_train_pred = model.predict(X_train)\\n            y_test_pred = model.predict(X_test)\\n            train_model_score = r2_score(y_train, y_train_pred)\\n            test_model_score = r2_score(y_test, y_test_pred)\\n\\n            report[list(models.keys())[i]] = test_model_score\\n\\n        return report\\n\\n    except Exception as e:\\n        raise Custom_Exception(e, sys)\\n', metadata={'source': 'test_repo/src/utils.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo/src/pipeline/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nfrom src.exception import Custom_Exception\\nfrom src.utils import load_object\\n\\n\\nclass PredictPipeline:\\n    def __init__(self) -> None:\\n        pass\\n\\n    def predict(self,features):\\n        \"\"\"\\n            Make predictions using a trained model on the provided features.\\n\\n            Args:\\n                features (numpy.ndarray): Input features for prediction.\\n\\n            Returns:\\n                float: Predicted value based on the input features.\\n\\n            Raises:\\n                Custom_Exception: If an error occurs during prediction, a Custom_Exception is raised.\\n        \"\"\"\\n        try:\\n            model_path = os.path.join(\\'artifacts\\',\\'model.pkl\\')\\n            preprocessor_path = os.path.join(\\'artifacts\\',\\'preprocessor.pkl\\')\\n            model = load_object(file_path = model_path)\\n            preprocessor = load_object(file_path = preprocessor_path)\\n            data_scaled = preprocessor.transform(features)\\n            preds = model.predict(data_scaled)\\n\\n            return preds\\n        except Exception as e:\\n            raise Custom_Exception(e,sys)\\n\\n\\nclass CustomData:\\n    def __init__(  self,\\n        gender: str,\\n        race_ethnicity: str,\\n        parental_level_of_education: str,\\n        lunch: str,\\n        test_preparation_course: str,\\n        reading_score: int,\\n        writing_score: int):\\n\\n        self.gender = gender\\n        self.race_ethnicity = race_ethnicity\\n        self.parental_level_of_education = parental_level_of_education\\n        self.lunch = lunch\\n        self.test_preparation_course = test_preparation_course\\n        self.reading_score = reading_score\\n        self.writing_score = writing_score\\n\\n    \\n    def get_data_as_data_frame(self):\\n        \"\"\"\\n            Create a pandas DataFrame from the attributes of the CustomData object.\\n\\n            Returns:\\n                pandas.DataFrame: A DataFrame containing the attributes as columns.\\n\\n            Raises:\\n                Custom_Exception: If an error occurs during DataFrame creation, a Custom_Exception is raised.\\n        \"\"\"\\n        try:\\n            custom_data_input_dict = {\\n                \"gender\": [self.gender],\\n                \"race_ethnicity\": [self.race_ethnicity],\\n                \"parental_level_of_education\": [self.parental_level_of_education],\\n                \"lunch\": [self.lunch],\\n                \"test_preparation_course\": [self.test_preparation_course],\\n                \"reading_score\": [self.reading_score],\\n                \"writing_score\": [self.writing_score],\\n            }\\n            return pd.DataFrame(custom_data_input_dict)\\n        \\n        except Exception as e:\\n            raise Custom_Exception(e,sys)', metadata={'source': 'test_repo/src/pipeline/predict_pipeline.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport pandas as pd\\nfrom src.exception import Custom_Exception\\nfrom src.logger import logging\\nfrom src.components.data_transformation import (\\n    DataTransformation,\\n    DataTransformationConfig,\\n)\\nfrom src.components.model_trainer import ModelTrainerConfig, ModelTrainer\\n\\nfrom sklearn.model_selection import train_test_split\\nfrom dataclasses import dataclass\\n\\n\\n@dataclass\\nclass DataIngestionConfig:\\n    train_data_path: str = os.path.join(\"artifacts\", \"train.csv\")\\n    test_data_path: str = os.path.join(\"artifacts\", \"test.csv\")\\n    raw_data_path: str = os.path.join(\"artifacts\", \"data.csv\")\\n\\n\\nclass DataIngestion:\\n    def __init__(self) -> None:\\n        self.ingestion_config = DataIngestionConfig()\\n\\n    def initiate_data_ingestion(self) -> tuple[str, str]:\\n        \"\"\"\\n            Read raw data from a specific data source, split it into a train set and test set, save them,\\n            and return the file paths to the train and test data.\\n\\n            Returns:\\n                tuple[str, str]: A tuple containing the file paths to the following elements in order:\\n                    - str: File path to the training data.\\n                    - str: File path to the testing data.\\n\\n            Raises:\\n                Custom_Exception: If an error occurs during data ingestion, a Custom_Exception is raised.\\n        \"\"\"\\n        logging.info(f\"Entered the data ingestion method\")\\n        try:\\n            file_path = f\"Data/student_data.csv\"\\n            df = pd.read_csv(file_path)\\n            logging.info(\\n                f\"Read the dataset as a dataframe from the local directory : {file_path}.\"\\n            )\\n            os.makedirs(\\n                os.path.dirname(self.ingestion_config.train_data_path), exist_ok=True\\n            )\\n            df.to_csv(self.ingestion_config.raw_data_path, index=False, header=True)\\n            logging.info(\"Train test split initiated.\")\\n            train_set, test_set = train_test_split(df, test_size=0.2, random_state=42)\\n            train_set.to_csv(\\n                self.ingestion_config.train_data_path, index=False, header=True\\n            )\\n            test_set.to_csv(\\n                self.ingestion_config.test_data_path, index=False, header=True\\n            )\\n            logging.info(\"Ingestion of the data is complete.\")\\n\\n            return (\\n                self.ingestion_config.train_data_path,\\n                self.ingestion_config.test_data_path,\\n            )\\n\\n        except Exception as e:\\n            raise Custom_Exception(sys, e)\\n\\n\\nif __name__ == \"__main__\":\\n    obj = DataIngestion()\\n    train_data, test_data = obj.initiate_data_ingestion()\\n\\n    data_transformation = DataTransformation()\\n    train_arr, test_arr, _ = data_transformation.initiate_data_transformation(\\n        train_data, test_data\\n    )\\n\\n    modeltrainer = ModelTrainer()\\n    print(\\n        modeltrainer.inititate_model_trainer(train_array=train_arr, test_array=test_arr)\\n    )\\n', metadata={'source': 'test_repo/src/components/data_ingestion.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import os\\nimport sys\\nimport numpy as np\\nfrom dataclasses import dataclass\\n\\nfrom catboost import CatBoostRegressor\\nfrom sklearn.ensemble import (\\n    AdaBoostRegressor,\\n    GradientBoostingRegressor,\\n    RandomForestRegressor,\\n)\\nfrom sklearn.linear_model import LinearRegression\\nfrom sklearn.neighbors import KNeighborsRegressor\\nfrom sklearn.tree import DecisionTreeRegressor, ExtraTreeRegressor\\nfrom xgboost import XGBRegressor\\n\\nfrom sklearn.metrics import r2_score\\n\\nfrom src.exception import Custom_Exception\\nfrom src.logger import logging\\n\\nfrom src.utils import save_object, evaluate_models\\n\\n\\n@dataclass\\nclass ModelTrainerConfig:\\n    trained_model_file_path = os.path.join(\"artifacts\", \"model.pkl\")\\n\\n\\nclass ModelTrainer:\\n    def __init__(self) -> None:\\n        self.model_trainer_config = ModelTrainerConfig()\\n\\n    def inititate_model_trainer(self, train_array: np.array, test_array: np.array) -> float:\\n        \\'\\'\\'\\n            This function splits the data into train and test sets, trains various machine learning models \\n            on the training data, selects the best-performing model, and saves it. It then evaluates the \\n            selected model on the test data and returns the R2 score.\\n\\n            Parameters:\\n                train_array (np.array): An array containing the training data, including features and labels.\\n                test_array (np.array): An array containing the test data, including features and labels.\\n\\n            Returns:\\n                float: The R2 score of the selected model on the test data.\\n\\n            Raises:\\n                Custom_Exception: If no best model with a score greater than or equal to 0.6 is found.\\n\\n            The function follows these steps:\\n            1. Splits the input data into training and test sets.\\n            2. Defines a set of machine learning models with default hyperparameters.\\n            3. Defines hyperparameter search spaces for some of the models.\\n            4. Evaluates the models on the training and test data, storing their performance in a dictionary.\\n            5. Selects the best-performing model based on the highest score.\\n            6. If the best model\\'s score is less than 0.6, it raises a Custom_Exception.\\n            7. Saves the best model to a specified file path.\\n            8. Uses the best model to make predictions on the test data and calculates the R2 score.\\n            9. Returns the R2 score.\\n        \\'\\'\\'\\n        try:\\n            logging.info(f\"Split training and test inout data.\")\\n            X_train, y_train, X_test, y_test = (\\n                train_array[:, :-1],\\n                train_array[:, -1],\\n                test_array[:, :-1],\\n                test_array[:, -1],\\n            )\\n            models = {\\n                \"Random Forest\": RandomForestRegressor(),\\n                \"Decision Tree\": DecisionTreeRegressor(),\\n                \"Gradient Boosting\": GradientBoostingRegressor(),\\n                \"Linear Regression\": LinearRegression(),\\n                \"XGBRegressor\": XGBRegressor(),\\n                \"CatBoosting Regressor\": CatBoostRegressor(verbose=False),\\n                \"AdaBoost Regressor\": AdaBoostRegressor(),\\n                \"Extratree Regressor\": ExtraTreeRegressor(),\\n                \"KNeighborsRegressor\": KNeighborsRegressor(),\\n            }\\n            params = {\\n                \"Decision Tree\": {\\n                    \"criterion\": [\\n                        \"squared_error\",\\n                        \"friedman_mse\",\\n                        \"absolute_error\",\\n                        \"poisson\",\\n                    ],\\n                    # \\'splitter\\':[\\'best\\',\\'random\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\'],\\n                },\\n                \"Extratree Regressor\": {\\n                    \"criterion\": [\\n                        \"squared_error\",\\n                        \"friedman_mse\",\\n                        \"absolute_error\",\\n                        \"poisson\",\\n                    ],\\n                    # \\'splitter\\':[\\'best\\',\\'random\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\'],\\n                },\\n                \"Random Forest\": {  #\\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\', \\'absolute_error\\', \\'poisson\\'],\\n                    # \\'max_features\\':[\\'sqrt\\',\\'log2\\',None],\\n                    \"n_estimators\": [8, 16, 32, 64, 128, 256]\\n                },\\n                \"Gradient Boosting\": {  #\\'loss\\':[\\'squared_error\\', \\'huber\\', \\'absolute_error\\', \\'quantile\\'],\\n                    \"learning_rate\": [0.1, 0.01, 0.05, 0.001],\\n                    \"subsample\": [0.6, 0.7, 0.75, 0.8, 0.85, 0.9],\\n                    # \\'criterion\\':[\\'squared_error\\', \\'friedman_mse\\'],\\n                    # \\'max_features\\':[\\'auto\\',\\'sqrt\\',\\'log2\\'],\\n                    \"n_estimators\": [8, 16, 32, 64, 128, 256],\\n                },\\n                \"Linear Regression\": {},\\n                \"KNeighborsRegressor\": {},\\n                \"XGBRegressor\": {\\n                    \"learning_rate\": [0.1, 0.01, 0.05, 0.001],\\n                    \"n_estimators\": [8, 16, 32, 64, 128, 256],\\n                },\\n                \"CatBoosting Regressor\": {\\n                    \"depth\": [6, 8, 10],\\n                    \"learning_rate\": [0.01, 0.05, 0.1],\\n                    \"iterations\": [30, 50, 100],\\n                },\\n                \"AdaBoost Regressor\": {\\n                    \"learning_rate\": [0.1, 0.01, 0.5, 0.001],\\n                    # \\'loss\\':[\\'linear\\',\\'square\\',\\'exponential\\'],\\n                    \"n_estimators\": [8, 16, 32, 64, 128, 256],\\n                },\\n            }\\n\\n            model_report: dict = evaluate_models(\\n                X_train=X_train,\\n                y_train=y_train,\\n                X_test=X_test,\\n                y_test=y_test,\\n                models=models,\\n                params=params,\\n            )\\n            # Getting the best model score from the dictionary\\n            best_model_score = max(sorted(model_report.values()))\\n\\n            # Getting the name of the best model from the dictionary\\n            best_model_name = list(model_report.keys())[\\n                list(model_report.values()).index(best_model_score)\\n            ]\\n            best_model = models[best_model_name]\\n\\n            if best_model_score < 0.6:\\n                raise Custom_Exception(f\"No best model found\")\\n            logging.info(\\n                f\"Best model : {best_model} with score : {best_model_score} found on both training and testing set.\"\\n            )\\n            # dumping the best model\\n            save_object(\\n                file_path=self.model_trainer_config.trained_model_file_path,\\n                obj=best_model,\\n            )\\n\\n            predicted = best_model.predict(X_test)\\n            r2_square = r2_score(y_test, predicted)\\n            logging.info(f\"R2 Score : {r2_square}\")\\n\\n            return r2_square\\n\\n        except Exception as e:\\n            raise Custom_Exception(e, sys)\\n', metadata={'source': 'test_repo/src/components/model_trainer.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='', metadata={'source': 'test_repo/src/components/__init__.py', 'language': <Language.PYTHON: 'python'>}),\n",
       " Document(page_content='import sys\\nimport os\\nimport numpy as np\\nimport pandas as pd\\nfrom sklearn.compose import ColumnTransformer\\nfrom sklearn.impute import SimpleImputer\\nfrom sklearn.pipeline import Pipeline\\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\\n\\nfrom dataclasses import dataclass\\n\\nfrom src.exception import Custom_Exception\\nfrom src.logger import logging\\nfrom src.utils import save_object\\n\\n\\n@dataclass\\nclass DataTransformationConfig:\\n    preprocessor_obj_file_path = os.path.join(\"artifacts\", \"preprocessor.pkl\")\\n\\n\\nclass DataTransformation:\\n    def __init__(self):\\n        self.data_transformation_config = DataTransformationConfig()\\n\\n    def get_data_transformer_object(self):\\n        \"\"\"\\n            Get a data transformation object that handles numerical and categorical columns.\\n\\n            Returns:\\n                ColumnTransformer: A ColumnTransformer object that preprocesses numerical columns\\n                using median imputation and standard scaling, and preprocesses categorical columns\\n                using most frequent imputation, one-hot encoding, and standard scaling.\\n\\n            Raises:\\n                Custom_Exception: If an error occurs while creating the data transformation object, a Custom_Exception is raised.\\n        \"\"\"\\n        try:\\n            numerical_columns = [\"writing_score\", \"reading_score\"]\\n            categorical_columns = [\\n                \"gender\",\\n                \"race_ethnicity\",\\n                \"parental_level_of_education\",\\n                \"lunch\",\\n                \"test_preparation_course\",\\n            ]\\n            num_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"median\")),\\n                    (\"scaler\", StandardScaler()),\\n                ]\\n            )\\n            cat_pipeline = Pipeline(\\n                steps=[\\n                    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\\n                    (\"one_hot_encoder\", OneHotEncoder()),\\n                    (\"scaler\", StandardScaler(with_mean=False)),\\n                ]\\n            )\\n            logging.info(\\n                f\"Numerical Columns : {numerical_columns}, standard scaling completed\"\\n            )\\n            logging.info(\\n                f\"Categorical Columns : {categorical_columns}, encoding completed\"\\n            )\\n\\n            preprocessor = ColumnTransformer(\\n                [\\n                    (\"num_pipeline\", num_pipeline, numerical_columns),\\n                    (\"cat_pipeline\", cat_pipeline, categorical_columns),\\n                ]\\n            )\\n\\n            return preprocessor\\n\\n        except Exception as e:\\n            raise Custom_Exception(e, sys)\\n\\n    def initiate_data_transformation(self, train_path: str, test_path: str) -> tuple:\\n        \"\"\"\\n            Read train and test data from CSV files, preprocess them using a data transformation object,\\n            and return the processed train and test arrays along with the file path to the preprocessing object.\\n\\n            Args:\\n                train_path (str): The file path to the training data CSV file.\\n                test_path (str): The file path to the testing data CSV file.\\n\\n            Returns:\\n                tuple: A tuple containing the following elements in order:\\n                    - numpy.ndarray: Processed training data array.\\n                    - numpy.ndarray: Processed testing data array.\\n                    - str: File path to the saved preprocessing object.\\n\\n            Raises:\\n                Custom_Exception: If an error occurs during data transformation, a Custom_Exception is raised.\\n\\n            Note:\\n                This function assumes that the target column name is \"math_score\" and that there are numerical\\n                columns named \"writing_score\" and \"reading_score\" in the data.\\n        \"\"\"\\n        try:\\n            train_df = pd.read_csv(train_path)\\n            test_df = pd.read_csv(test_path)\\n            logging.info(f\"Read the train and test data.\")\\n            logging.info(f\"Obtaining the preprocessing object.\")\\n\\n            preprocessing_obj = self.get_data_transformer_object()\\n\\n            target_column_name = \"math_score\"\\n            numerical_columns = [\"writing_score\", \"reading_score\"]\\n\\n            input_feature_train_df = train_df.drop(columns=[target_column_name], axis=1)\\n            target_feature_train_df = train_df[target_column_name]\\n            input_feature_test_df = test_df.drop(columns=[target_column_name], axis=1)\\n            target_feature_test_df = test_df[target_column_name]\\n\\n            logging.info(\\n                f\"Applying the preprocessing object on the training dataframe and testing dataframe.\"\\n            )\\n\\n            input_feature_train_arr = preprocessing_obj.fit_transform(\\n                input_feature_train_df\\n            )\\n            input_feature_test_arr = preprocessing_obj.transform(input_feature_test_df)\\n\\n            train_arr = np.c_[\\n                input_feature_train_arr, np.array(target_feature_train_df)\\n            ]\\n            test_arr = np.c_[input_feature_test_arr, np.array(target_feature_test_df)]\\n            logging.info(f\"Saving the preprocessing object\")\\n\\n            save_object(\\n                file_path=self.data_transformation_config.preprocessor_obj_file_path,\\n                obj=preprocessing_obj,\\n            )\\n\\n            return (\\n                train_arr,\\n                test_arr,\\n                self.data_transformation_config.preprocessor_obj_file_path,\\n            )\\n\\n        except Exception as e:\\n            raise Custom_Exception(e, sys)\\n', metadata={'source': 'test_repo/src/components/data_transformation.py', 'language': <Language.PYTHON: 'python'>})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chunkings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents_splitter = RecursiveCharacterTextSplitter.from_language(language = Language.PYTHON,\n",
    "                                                             chunk_size = 2000,\n",
    "                                                             chunk_overlap = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = documents_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings=OpenAIEmbeddings(disallowed_special=())\n",
    "# disallowed special removes the special characters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# knowledge base (Chroma DB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, embedding=embeddings, persist_directory='./data')\n",
    "vectordb.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LLM Wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm = ChatOpenAI(model_name=\"gpt-4\")\n",
    "llm = ChatOpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ConversationSummaryMemory(llm=llm, memory_key = \"chat_history\", return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = ConversationalRetrievalChain.from_llm(llm, retriever=vectordb.as_retriever(search_type=\"mmr\", search_kwargs={\"k\":3}), memory=memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q&A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"what is DataIngestion class?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The purpose of the DataIngestion class is to read raw data from a specific data source, split it into a train set and test set, save them as CSV files, and return the file paths to the train and test data. The class also handles exceptions during data ingestion by raising a Custom_Exception if an error occurs.\n"
     ]
    }
   ],
   "source": [
    "result = qa(question)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
